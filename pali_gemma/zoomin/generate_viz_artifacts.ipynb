{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "PALIGEMMA_IMAGE_SIZE = 896\n",
    "PALIGEMMA_PATCH_SIZE = 14\n",
    "PALIGEMMA_IMAGE_TOKEN_ID = 257152\n",
    "\n",
    "dataset = load_dataset(\"arnaudstiegler/v2_synthetic_us_passports_easy\")\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "dataset = load_dataset(\"arnaudstiegler/v2_synthetic_us_passports_easy\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "base_model = \"google/paligemma-3b-pt-896\"\n",
    "adapter_model = \"arnaudstiegler/paligemma-3b-pt-896-us-passports-lora-adapters\"\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    base_model, quantization_config=bnb_config\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapter_model).to(device)\n",
    "processor = AutoProcessor.from_pretrained(base_model)\n",
    "\n",
    "model = model.eval().to(\"cuda\")\n",
    "\n",
    "sample = dataset['test'][0]\n",
    "image = sample[\"image\"].convert(\"RGB\")\n",
    "inputs = processor(text=\"Process \", images=image, return_tensors=\"pt\")\n",
    "inputs = {\n",
    "    \"input_ids\": inputs[\"input_ids\"].to(\"cuda\"),\n",
    "    \"attention_mask\": inputs[\"attention_mask\"].to(\"cuda\"),\n",
    "    \"pixel_values\": inputs[\"pixel_values\"].to(\"cuda\"),\n",
    "}\n",
    "out = model.generate(\n",
    "    **inputs,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    return_dict_in_generate=True,\n",
    "    max_new_tokens=164,\n",
    ")\n",
    "\n",
    "img_tokens = torch.where(inputs[\"input_ids\"] == PALIGEMMA_IMAGE_TOKEN_ID)\n",
    "\n",
    "grid = {}\n",
    "for k in range((PALIGEMMA_IMAGE_SIZE // PALIGEMMA_PATCH_SIZE) * (PALIGEMMA_IMAGE_SIZE // PALIGEMMA_PATCH_SIZE)):\n",
    "    col_idx = k % (PALIGEMMA_IMAGE_SIZE // PALIGEMMA_PATCH_SIZE)\n",
    "    row_idx = k // (PALIGEMMA_IMAGE_SIZE // PALIGEMMA_PATCH_SIZE)\n",
    "    # In format x1,y1,x2,y2\n",
    "    grid[k] = [\n",
    "        col_idx * PALIGEMMA_PATCH_SIZE,\n",
    "        row_idx * PALIGEMMA_PATCH_SIZE,\n",
    "        (col_idx + 1) * PALIGEMMA_PATCH_SIZE,\n",
    "        (row_idx + 1) * PALIGEMMA_PATCH_SIZE,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = []\n",
    "# We start from 1 because we're skipping the original self-attention across all tokens\n",
    "# TODO: maybe we could bring it back\n",
    "for idx in range(1, len(out.attentions)):\n",
    "    # Stack across layers\n",
    "    img_attention = torch.stack([x for x in out.attentions[idx]], axis=-1)\n",
    "\n",
    "    # Average the attentions across layers\n",
    "    avg_img_attention = torch.mean(img_attention, axis=-1)\n",
    "    # Average the attention across heads\n",
    "    avg_img_attention = torch.mean(avg_img_attention, axis=1)\n",
    "\n",
    "    # Only take the attention scores corresponding to the image\n",
    "    img_attentions = avg_img_attention[:, :, : img_tokens[1][-1]]\n",
    "\n",
    "    # Only look at absolute value of the attention\n",
    "    att_score = torch.abs(img_attentions)\n",
    "    # Take top-k image patches\n",
    "    patch_indices = torch.topk(att_score, k=5, dim=-1).indices\n",
    "\n",
    "    # Given a patch index, and a grid, retrieve the location of the different image patches on the image\n",
    "    top_bbox = [\n",
    "        tuple(grid[patch_idx.item()]) for patch_idx in patch_indices.flatten()\n",
    "    ]\n",
    "    top += top_bbox\n",
    "\n",
    "width, height = image.size\n",
    "min_x, min_y, max_x, max_y = normalized_bbox\n",
    "factor_width = width / PALIGEMMA_IMAGE_SIZE\n",
    "factor_height = height / PALIGEMMA_IMAGE_SIZE\n",
    "\n",
    "# Get coordinates of the bbox that covers all top-k image patches (should be the reason of interest)\n",
    "min_x, min_y, max_x, max_y = PALIGEMMA_IMAGE_SIZE, PALIGEMMA_IMAGE_SIZE, 0, 0\n",
    "for token, boxes in zip(out.sequences, top):\n",
    "    for bbox in boxes:\n",
    "        resized_bbox = [\n",
    "            factor_width * bbox[0],\n",
    "            factor_height * bbox[1],\n",
    "            factor_width * bbox[2],\n",
    "            factor_height * bbox[3],\n",
    "        ]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
